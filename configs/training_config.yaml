model:
  base_model_name: "meta-llama/Llama-3.1-70B"
  num_parallel_chains: 8
  reasoning_depth: 5
  hidden_size: 8192
  intermediate_size: 28672
  num_attention_heads: 64
  num_key_value_heads: 8
  num_hidden_layers: 80
  vocab_size: 128256
  max_position_embeddings: 131072
  rope_theta: 500000.0
  attention_dropout: 0.0
  use_flash_attention: true
  synthesis_temperature: 0.7
  validation_threshold: 0.85
  gradient_checkpointing: true

training:
  output_dir: "./outputs/dps_model"
  num_epochs: 3
  batch_size: 4
  eval_batch_size: 8
  gradient_accumulation_steps: 4
  total_batch_size: 32
  learning_rate: 5e-5
  warmup_steps: 1000
  weight_decay: 0.01
  gradient_clipping: 1.0
  mixed_precision: "bf16"
  use_deepspeed: true
  zero_stage: 3
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  use_wandb: true
  seed: 42

data:
  train_data_path: "./data/train.json"
  eval_data_path: "./data/eval.json"
  test_data_path: "./data/test.json"
  max_length: 2048
  validation_required: true

deepspeed:
  enabled: true
  config_path: "./configs/deepspeed_config.json"
  
inference:
  tensor_parallel_size: 1
  max_batch_size: 32
  use_vllm: true
  gpu_memory_utilization: 0.9
  
evaluation:
  metrics:
    - logical_consistency
    - reasoning_depth
    - convergence_quality
    - scientific_accuracy
  domains:
    - physics
    - chemistry
    - biology
    - mathematics
    - general
  output_dir: "./evaluation_results"